{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a860da5-768a-4326-a6c8-7300b9797340",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8bea6-c821-4a5f-86ea-6d9db82c69ec",
   "metadata": {},
   "source": [
    "## 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6418e-4cb5-4743-b11e-1bbfd85fec79",
   "metadata": {},
   "source": [
    "计算图通常包含两种元素，一个是 tensor，另一个是 Function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06493c1c-ba3a-4ffa-82c4-ef129e268eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4011, -0.9069], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x  = torch.randn(2, requires_grad=True, device='cuda')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8adbc563-e869-417a-a9d4-441fa0927b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40107426047325134, -0.906868040561676]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d240d416-83ac-47a0-9a3c-5213dd19a320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3891, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = a.exp()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ecc41-f7e6-4d4e-9530-0025f616ff17",
   "metadata": {},
   "source": [
    "在我们做正向传播的过程中，除了执行 forward() 操作之外，还会同时会为反向传播做一些准备，为反向计算图添加 Function 节点。在上边这个例子中，变量 b 在反向传播中所需要进行的操作是 `<ExpBackward>`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fad8e1-f9fd-46c5-bdb0-1b862e3d02b8",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## 一个具体的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204faac-bf3f-4565-b103-8e7c4a4080ed",
   "metadata": {},
   "source": [
    "**定义一个前向传播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6b38090-8020-4058-b54b-105638166386",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones([2, 2], requires_grad=True)\n",
    "w1 = torch.tensor(2.0, requires_grad=True)\n",
    "w2 = torch.tensor(3.0, requires_grad=True)\n",
    "w3 = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "l1 = inp * w1\n",
    "l2 = l1 + w2\n",
    "l3 = l1 * w3\n",
    "l4 = l2 * l3\n",
    "loss = l4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90a299b1-c3e2-411f-96da-d1020ed91427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) None None\n"
     ]
    }
   ],
   "source": [
    "print(w1.data, w1.grad, w1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01fd6b8c-c270-4ab7-a9b3-9bf084f3fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]]) None <MulBackward0 object at 0x7f40edae4220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-c589a7d06409>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(l1.data, l1.grad, l1.grad_fn)\n"
     ]
    }
   ],
   "source": [
    "print(l1.data, l1.grad, l1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bedb03d8-d21d-4ac3-9e9b-b8061da20506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.) None <MeanBackward0 object at 0x7f40edae4a00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-e63e1720befd>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(loss.data, loss.grad, loss.grad_fn)\n"
     ]
    }
   ],
   "source": [
    "print(loss.data, loss.grad, loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43c0cee7-6a42-47d2-955b-cb35dd942c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09599b7-977c-4c60-ab25-e27f3f5ef08a",
   "metadata": {},
   "source": [
    "**反向传播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "401fcc02-8a66-4d20-8dec-04ab18ba58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fcba257-645a-4a0c-9379-bca5c7502867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.) tensor(8.) tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "print(w1.grad, w2.grad, w3.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1ae21-e576-4523-915c-f740dac57b1d",
   "metadata": {},
   "source": [
    "`w1` 等是叶子节点，导数被保留下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c75aea1c-0117-4649-b1cf-233a9c5fbe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-793453e07697>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(l1.grad, l2.grad, l3.grad, l4.grad, loss.grad)\n"
     ]
    }
   ],
   "source": [
    "print(l1.grad, l2.grad, l3.grad, l4.grad, loss.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb07d61-2cb5-41b6-88ba-33ba3838e6f8",
   "metadata": {},
   "source": [
    "`l1` 等是非叶子节点，导数没有被保留下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26b8537a-46fd-40f0-a0f7-dc989f3eff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14., 14.],\n",
      "        [14., 14.]])\n"
     ]
    }
   ],
   "source": [
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f5163-7790-4e67-a64c-66339337e09f",
   "metadata": {},
   "source": [
    "**一些概念**\n",
    "\n",
    "1. `tensor.is_leaf` 用来判断是否是叶子张量；\n",
    "2. 在 BP 中，只有 `is_leaf = True` 的时候，需要求导的张量的导数结果才会被最后保留下来；\n",
    "3. 由用户创建的 tensor 是一个叶子节点，而由其他运算操作产生的 tensor 则不是；\n",
    "4. 将 `requires_grad = False` 的 tensor，约定俗成地归为叶子张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb1f4c74-5593-4c45-9946-5b18dc94cf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones([2, 2], requires_grad=False)\n",
    "print(a.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a37ab-caea-43c0-81c2-b6a44b203b2c",
   "metadata": {},
   "source": [
    "**保留中间变量的导数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db446ff-3852-45f6-a0f8-2b572ab27de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones([2, 2], requires_grad=False)\n",
    "w1 = torch.tensor(2.0, requires_grad=True)\n",
    "w2 = torch.tensor(3.0, requires_grad=True)\n",
    "w3 = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "l1 = input * w1\n",
    "l2 = l1 + w2\n",
    "l3 = l1 * w3\n",
    "l4 = l2 * l3\n",
    "loss = l4.mean()\n",
    "\n",
    "# 使用 tensor.retain_grad() 保留中间变量的导数\n",
    "l1.retain_grad()\n",
    "l4.retain_grad()\n",
    "loss.retain_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8422a2d1-dba3-4d38-85ac-93c36c460224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ba9482b-cf4b-4bfb-8275-b334f78d860e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbd1731c-967f-43b9-aa1c-011f8cdf5ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7.],\n",
       "        [7., 7.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0dd21-4074-4c0c-bb2a-5f39d3eb906f",
   "metadata": {},
   "source": [
    "如果只想 debug，而不需要保存中间变量的导数信息，可以使用 `tensor.register_hook` ([pytorch中的钩子（Hook）有何作用？](https://www.zhihu.com/question/61044004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460728d2-a4df-4813-bad3-6918347acaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss grad:  tensor(1.)\n",
      "l4 grad:  tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "l1 grad:  tensor([[7., 7.],\n",
      "        [7., 7.]])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-a2af1cf65e2c>:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(loss.grad)\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones([2, 2], requires_grad=False)\n",
    "w1 = torch.tensor(2.0, requires_grad=True)\n",
    "w2 = torch.tensor(3.0, requires_grad=True)\n",
    "w3 = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "l1 = input * w1\n",
    "l2 = l1 + w2\n",
    "l3 = l1 * w3\n",
    "l4 = l2 * l3\n",
    "loss = l4.mean()\n",
    "\n",
    "# 使用 tensor.register_hook\n",
    "l1.register_hook(lambda grad: print('l1 grad: ', grad))\n",
    "l4.register_hook(lambda grad: print('l4 grad: ', grad))\n",
    "loss.register_hook(lambda grad: print('loss grad: ', grad))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7bd31-47f7-4c0e-97aa-f2c2f2671f82",
   "metadata": {},
   "source": [
    "loss 的 grad 在 print 完之后就被清除掉了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea4d2e-3ad5-4d30-b343-a67fe4312e66",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## inplace 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a797e-04b4-4083-994d-182eac7cc8cb",
   "metadata": {},
   "source": [
    "> 官方不推荐使用 inplace 的方式操作 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f2fe77b-671a-4264-b7ab-7fae55901531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 5]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3],[4,5,6]])\n",
    "result = A + [[2], [3]]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3901e4-e3c8-4c83-bf57-7e149ddd01c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139917664196864\n",
      "139917664196736\n"
     ]
    }
   ],
   "source": [
    "# 情景 1\n",
    "a = torch.tensor([3.0, 1.0])\n",
    "print(id(a))\n",
    "a = a.exp()\n",
    "print(id(a))\n",
    "# 在这个过程中 a.exp() 生成了一个新的对象，然后再让 a\n",
    "# 指向它的地址，所以这不是个 inplace 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aec3ce87-2137-4cfb-ade6-342fb740b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139918090325440\n",
      "139918090325440 tensor([10.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# 情景 2\n",
    "a = torch.tensor([3.0, 1.0])\n",
    "print(id(a))\n",
    "a[0] = 10\n",
    "print(id(a), a)\n",
    "# inplace 操作，内存地址没变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad2890-f26b-4f36-ae36-2c20671f1286",
   "metadata": {},
   "source": [
    "`tensor._version` 检测 tensor 有没有发生 inplace 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be009b9a-ef0e-4179-9f28-bb0e9df07dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 3.0], requires_grad=True)\n",
    "b = a + 2\n",
    "print(b._version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a15253b-a450-41c2-b751-ac13b6016edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "loss = (b * b).mean()\n",
    "b[0] = 1000.0\n",
    "print(b._version)  # 每次检测到 b 发生 inplace 操作时，_version 就会 +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030427a1-f9db-49e3-a8ae-cb4fd39b968f",
   "metadata": {},
   "source": [
    "反向传播时发现 b 已经 inplace 修改了，报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34446bc5-24fd-4c45-9fe9-8840feeaf240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2]], which is output 0 of AddBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2]], which is output 0 of AddBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738445-333d-4ba2-9a2b-b8f0e47f75d9",
   "metadata": {},
   "source": [
    "需要求导的叶子节点要求更严，不用等反向传播，一旦创建后，做 inplace 操作就会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a62c6e7e-c90b-46db-b757-caecd1e55b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-a8f12ddf6a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "a.add_(10.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f3cfb-81c9-450e-b706-c9dd809864f2",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a102a7-482f-403a-b469-efc95e36f3a3",
   "metadata": {},
   "source": [
    "- [PyTorch 的 Autograd](https://zhuanlan.zhihu.com/p/69294347)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
